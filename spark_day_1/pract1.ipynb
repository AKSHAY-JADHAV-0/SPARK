{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f27717dd-207d-4b32-b190-a45ae671dbc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (3.4.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: findspark in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e308e264-34c9-4440-aed6-c93c8898fad6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fca62ac-6c3c-4f04-8fa2-d959b340e4c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PySpark is the Spark API for Python. In this lab, we use PySpark to initialize the spark context. \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a5c2f3-870d-4042-b926-d15949043f33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/05 06:30:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Creating a spark context class\n",
    "sc = SparkContext()\n",
    "\n",
    "# Creating a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark DataFrames basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4c25dd3-815f-453d-98d1-ca6bbd2d7680",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession is active and ready to use.\n"
     ]
    }
   ],
   "source": [
    "if 'spark' in locals() and isinstance(spark, SparkSession):\n",
    "    print(\"SparkSession is active and ready to use.\")\n",
    "else:\n",
    "    print(\"SparkSession is not active. Please create a SparkSession.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b41757f3-e8d8-4e47-ad60-394ec5378658",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = range(1,30)\n",
    "# print first element of iterator\n",
    "print(data[0])\n",
    "len(data)\n",
    "xrangeRDD = sc.parallelize(data, 4)\n",
    "\n",
    "# this will let us know that we created an RDD\n",
    "xrangeRDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d65e4c3-5020-4d55-a7ef-3cfcec598457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subRDD = xrangeRDD.map(lambda x: x-1)\n",
    "filteredRDD = subRDD.filter(lambda x : x<10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7c4f774-c846-42be-9d02-3e06026bdb1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(filteredRDD.collect())\n",
    "filteredRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "558e04ff-bafa-425e-9523-f425b88b5785",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt1:  0.8319389820098877\n",
      "dt2:  0.22359561920166016\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "test = sc.parallelize(range(1,50000),4)\n",
    "test.cache()\n",
    "\n",
    "t1 = time.time()\n",
    "# first count will trigger evaluation of count *and* cache\n",
    "count1 = test.count()\n",
    "dt1 = time.time() - t1\n",
    "print(\"dt1: \", dt1)\n",
    "\n",
    "\n",
    "t2 = time.time()\n",
    "# second count operates on cached data only\n",
    "count2 = test.count()\n",
    "dt2 = time.time() - t2\n",
    "print(\"dt2: \", dt2)\n",
    "\n",
    "#test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2df3de6d-c155-4904-9524-c884c82ad95b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyterlab-jadhavaksha3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd7e00d5b50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d90328ea-f401-458c-b112-aa86b3890f66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    73  100    73    0     0    839      0 --:--:-- --:--:-- --:--:--   839\n"
     ]
    }
   ],
   "source": [
    "# Download the data first into a local `people.json` file\n",
    "!curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/people.json >> people.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d08ca52-dbab-44d9-aefc-75d417dcd253",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(age=None, name='Michael'), Row(age=30, name='Andy'), Row(age=19, name='Justin'), Row(age=None, name='Michael'), Row(age=30, name='Andy'), Row(age=19, name='Justin')]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset into a spark dataframe using the `read.json()` function\n",
    "df = spark.read.json(\"people.json\").cache()\n",
    "\n",
    "print(df.collect())\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b26ca66-74da-4bc5-874c-f8f417a7c241",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the dataframe as well as the data schema\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b68ca3e-e075-4fed-9d56-71aa4e803b60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f748eed-0c9d-4131-9a47-37e4fd9b98a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select and show basic data columns\n",
    "\n",
    "df.select(\"name\").show()\n",
    "df.select(df[\"name\"]).show()\n",
    "spark.sql(\"SELECT name FROM people\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd3e3500-683e-4934-aee4-3e68f99f2597",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform basic filtering\n",
    "\n",
    "df.filter(df[\"age\"] > 21).show()\n",
    "spark.sql(\"SELECT age, name FROM people WHERE age > 21\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d64bbb86-2c83-46cb-b44c-e4522f3c113a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    2|\n",
      "|null|    2|\n",
      "|  30|    2|\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:=====================================================>  (72 + 3) / 75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    2|\n",
      "|null|    0|\n",
      "|  30|    2|\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Perfom basic aggregation of data\n",
    "\n",
    "df.groupBy(\"age\").count().show()\n",
    "spark.sql(\"SELECT age, COUNT(age) as count FROM people GROUP BY age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b116d813-627f-4e94-a098-fda06edbface",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyterlab-jadhavaksha3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd7e00d5b50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da163513-8445-43fc-bff0-2cb9e2bfc775",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48]\n"
     ]
    }
   ],
   "source": [
    "data = range(1, 50)\n",
    "\n",
    "RDD = sc.parallelize(data)\n",
    "\n",
    "# Filter the even numbers using modulus\n",
    "Even = RDD.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "print(Even.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cfb63152-2cb0-41e2-b528-6b0b23d09f10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   136  100   136    0     0   1789      0 --:--:-- --:--:-- --:--:--  1789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/05 07:40:53 WARN execution.CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(age=25, name='Michael'), Row(age=24, name='Andy'), Row(age=19, name='Justin'), Row(age=26, name='George'), Row(age=30, name='Jeff'), Row(age=24, name='Andy'), Row(age=19, name='Justin'), Row(age=26, name='George'), Row(age=30, name='Jeff'), Row(age=24, name='Andy'), Row(age=19, name='Justin'), Row(age=26, name='George'), Row(age=30, name='Jeff'), Row(age=24, name='Andy'), Row(age=19, name='Justin'), Row(age=26, name='George'), Row(age=30, name='Jeff'), Row(age=24, name='Andy'), Row(age=19, name='Justin'), Row(age=26, name='George'), Row(age=30, name='Jeff'), Row(age=24, name='Andy'), Row(age=19, name='Justin'), Row(age=26, name='George'), Row(age=30, name='Jeff')] 25\n",
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 25|Michael|\n",
      "| 24|   Andy|\n",
      "| 19| Justin|\n",
      "| 26| George|\n",
      "| 30|   Jeff|\n",
      "| 24|   Andy|\n",
      "| 19| Justin|\n",
      "| 26| George|\n",
      "| 30|   Jeff|\n",
      "| 24|   Andy|\n",
      "| 19| Justin|\n",
      "| 26| George|\n",
      "| 30|   Jeff|\n",
      "| 24|   Andy|\n",
      "| 19| Justin|\n",
      "| 26| George|\n",
      "| 30|   Jeff|\n",
      "| 24|   Andy|\n",
      "| 19| Justin|\n",
      "| 26| George|\n",
      "+---+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+\n",
      "|average_age|\n",
      "+-----------+\n",
      "|      24.76|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/people2.json >> people2.json\n",
    "\n",
    "df = spark.read.json(\"people2.json\")\n",
    "\n",
    "df.cache()\n",
    "\n",
    "print(df.collect(), df.count())\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.createOrReplaceTempView(\"people2\")\n",
    "\n",
    "spark.sql(\"SELECT AVG(age) AS average_age FROM people2\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3b837ef-7e4c-4c42-9568-6de00e7b4cab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d13e9a-6cca-4f2b-925d-b0e429deb181",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
