{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "518627dc-752a-4c38-a238-c8b76ccce0c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install PySpark version 3.1.2 silently\n",
    "!pip install pyspark==3.1.2 -q\n",
    "# Install findSpark silently\n",
    "!pip install findspark -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "871d2f3b-058b-4577-8106-66c42e06309e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Suppressing warnings by defining a function 'warn' that does nothing\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "# Importing the 'warnings' module to handle warnings\n",
    "import warnings\n",
    "\n",
    "# Overriding the 'warn' function in the 'warnings' module with the defined function to suppress warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "# Filtering out all warnings to be ignored\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FindSpark simplifies the process of using Apache Spark with Python\n",
    "# Importing the 'findspark' module\n",
    "import findspark\n",
    "\n",
    "# Initializing FindSpark to locate Spark installation\n",
    "findspark.init()\n",
    "\n",
    "# Importing SparkSession from pyspark.sql module\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "115336aa-7789-4199-bbdc-4141381f9c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 10:10:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark context\n",
    "sc = SparkContext(appName=\"RetailStoreSalesAnalysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0e849df-b120-49d6-860f-6754e0c696c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-15 10:10:29--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/XXlNzqYcxqkTbllc-tL_0w/Retailsales.csv\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 47593992 (45M) [text/csv]\n",
      "Saving to: ‘Retailsales.csv.2’\n",
      "\n",
      "Retailsales.csv.2   100%[===================>]  45.39M  36.7MB/s    in 1.2s    \n",
      "\n",
      "2024-11-15 10:10:31 (36.7 MB/s) - ‘Retailsales.csv.2’ saved [47593992/47593992]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/XXlNzqYcxqkTbllc-tL_0w/Retailsales.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5494a4d1-be77-4d62-8b8c-981093e03aed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['product_id,store_id,date,sales,revenue,stock,price,promo_type_1,promo_bin_1,promo_type_2', 'P0001,S0002,1/2/2017,0,0,8,6.25,PR14,,PR03', 'P0001,S0012,1/2/2017,1,5.3,0,6.25,PR14,,PR03', 'P0001,S0013,1/2/2017,2,10.59,0,6.25,PR14,,PR03', 'P0001,S0023,1/2/2017,0,0,6,6.25,PR14,,PR03']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "raw_data = sc.textFile(\"Retailsales.csv\")\n",
    "\n",
    "print(raw_data.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2a52800-f712-4ca3-8615-d3e1c9c0c9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse and Clean Data\n",
    "def parse_line(line):\n",
    "    # Split the line by comma to get fields\n",
    "    fields = line.split(\",\")\n",
    "    # Return a dictionary with parsed fields\n",
    "    return {\n",
    "        'product_id': fields[0],\n",
    "        'store_id': fields[1],\n",
    "        'date': fields[2],\n",
    "        'sales': float(fields[3]),\n",
    "        'revenue': float(fields[4]),\n",
    "        'stock': float(fields[5]),\n",
    "        'price': float(fields[6]),\n",
    "        'promo_type_1': fields[7],\n",
    "        'promo_type_2': fields[9]\n",
    "    }\n",
    "\n",
    "# Remove the header line\n",
    "header = raw_data.first()\n",
    "\n",
    "raw_data_no_header = raw_data.filter(lambda line: line != header)\n",
    "\n",
    "# Parse the lines into a structured format\n",
    "parsed_data = raw_data_no_header.map(parse_line)\n",
    "parsed_data = parsed_data.filter(lambda x: x is not None)\n",
    "\n",
    "\n",
    "# Filter out records with missing or invalid data\n",
    "cleaned_data = parsed_data.filter(lambda x: x['sales'] > 0 and x['price'] > 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "253822ad-64aa-42a4-bcd2-e815af669e32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P0001,S0002,1/2/2017,0,0,8,6.25,PR14,,PR03', 'P0001,S0012,1/2/2017,1,5.3,0,6.25,PR14,,PR03', 'P0001,S0013,1/2/2017,2,10.59,0,6.25,PR14,,PR03', 'P0001,S0023,1/2/2017,0,0,6,6.25,PR14,,PR03', 'P0001,S0025,1/2/2017,0,0,1,6.25,PR14,,PR03']\n"
     ]
    }
   ],
   "source": [
    "print(raw_data_no_header.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70538a27-23d2-4efa-b410-ff800c7b7720",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in cleaned_data: 2\n"
     ]
    }
   ],
   "source": [
    "# Check the number of partitions\n",
    "print(f\"Number of partitions in cleaned_data: {cleaned_data.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7f1fd74-afb5-45d6-b969-3d6e80370a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in each partition:\n",
      "Partition 0: 97534 records\n",
      "Partition 1: 99127 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Function to count the number of records in each partition\n",
    "def count_in_partition(index, iterator):\n",
    "    count = sum(1 for _ in iterator)\n",
    "    yield (index, count)\n",
    "\n",
    "# Get the count of records in each partition\n",
    "partitions_info = cleaned_data.mapPartitionsWithIndex(count_in_partition).collect()\n",
    "print(\"Number of records in each partition:\")\n",
    "for partition, count in partitions_info:\n",
    "    print(f\"Partition {partition}: {count} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "982678a6-2546-4306-8980-ef9901920c5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in cleaned_data: 2\n"
     ]
    }
   ],
   "source": [
    "# Aggregation 1: Total Sales and Revenue per Product\n",
    "sales_revenue_per_product = cleaned_data.map(lambda x: (x['product_id'], (x['sales'], x['revenue']))) \\\n",
    "                                        .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "print(f\"Number of partitions in cleaned_data: {cleaned_data.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22c8eb61-9758-45d8-85f3-055f22bfdad7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregation 2: Total Sales and Revenue per Store\n",
    "sales_revenue_per_store = cleaned_data.map(lambda x: (x['store_id'], (x['sales'], x['revenue']))) \\\n",
    "                                      .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "432e29d3-c59a-42bf-8115-be07bf31c949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregation 3: Average Price per Product\n",
    "total_price_count_per_product = cleaned_data.map(lambda x: (x['product_id'], (x['price'], 1))) \\\n",
    "                                            .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "average_price_per_product = total_price_count_per_product.mapValues(lambda x: x[0] / x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7365fcb8-61d5-46c8-a50e-e39b9c75d715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregation 4: Sales and Revenue per Promotion Type\n",
    "sales_revenue_per_promo_1 = cleaned_data.map(lambda x: (x['promo_type_1'], (x['sales'], x['revenue']))) \\\n",
    "                                        .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "sales_revenue_per_promo_2 = cleaned_data.map(lambda x: (x['promo_type_2'], (x['sales'], x['revenue']))) \\\n",
    "                                        .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccd894ff-5af1-426e-a0c1-1ad825365ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregation 5: Stock Analysis per Product\n",
    "stock_per_product = cleaned_data.map(lambda x: (x['product_id'], x['stock'])) \\\n",
    "                                .reduceByKey(lambda a, b: a + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f45b4609-428a-4df3-b155-467b92251a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o166.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/resources/BD0231EN/labs/sales_revenue_per_product already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:287)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1168/2719208006.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save results to HDFS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msales_revenue_per_product\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sales_revenue_per_product\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msales_revenue_per_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sales_revenue_per_store\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maverage_price_per_product\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"average_price_per_product\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msales_revenue_per_promo_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sales_revenue_per_promo_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.3/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.3/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.3/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o166.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/resources/BD0231EN/labs/sales_revenue_per_product already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:287)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "# Save results to HDFS\n",
    "sales_revenue_per_product.saveAsTextFile(\"sales_revenue_per_product\")\n",
    "sales_revenue_per_store.saveAsTextFile(\"sales_revenue_per_store\")\n",
    "average_price_per_product.saveAsTextFile(\"average_price_per_product\")\n",
    "sales_revenue_per_promo_1.saveAsTextFile(\"sales_revenue_per_promo_1\")\n",
    "sales_revenue_per_promo_2.saveAsTextFile(\"sales_revenue_per_promo_2\")\n",
    "stock_per_product.saveAsTextFile(\"stock_per_product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd748e-dc27-4a46-bcfb-5fd9b1c56113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(\"Total Sales and Revenue per Product:\")\n",
    "print(\"=\" * 35)\n",
    "for product in sales_revenue_per_product.collect():\n",
    "    # Create the format string with appropriate padding\n",
    "    format_string = f\"{{:<5}} | {{:<9}} | {{:<9}}\"\n",
    "\n",
    "    # Print the values using the format string\n",
    "    print(format_string.format(str(product[0]), str(round(product[1][0],2)), str(round(product[1][1],2))))\n",
    "\n",
    "print(\"\\n\\nTotal Sales and Revenue per Store:\")\n",
    "print(\"=\" * 35)\n",
    "for store in sales_revenue_per_store.collect():\n",
    "    format_string = f\"{{:<5}} | {{:<9}} | {{:<9}}\"\n",
    "    print(format_string.format(str(store[0]), str(round(store[1][0],2)), str(round(store[1][1],2))))\n",
    "\n",
    "print(\"\\n\\nAverage Price per Product:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "for product in average_price_per_product.collect():\n",
    "    format_string = f\"{{:<5}} | {{:<9}}\"\n",
    "    print(format_string.format(str(product[0]), str(round(product[1],2))))\n",
    "\n",
    "print(\"\\n\\nSales and Revenue per Promotion Type 1:\")\n",
    "print(\"=\" * 40)\n",
    "for promo in sales_revenue_per_promo_1.collect():\n",
    "    format_string = f\"{{:<5}} | {{:<9}} | {{:<9}}\"\n",
    "    print(format_string.format(str(promo[0]), str(round(promo[1][0],2)), str(round(promo[1][1],2))))\n",
    "\n",
    "print(\"\\n\\nSales and Revenue per Promotion Type 2:\")\n",
    "print(\"=\" * 40)\n",
    "for promo in sales_revenue_per_promo_2.collect():\n",
    "    format_string = f\"{{:<5}} | {{:<9}} | {{:<9}}\"\n",
    "\n",
    "    print(format_string.format(str(promo[0]), str(round(promo[1][0],2)), str(round(promo[1][1],2))))\n",
    "\n",
    "print(\"\\n\\nStock per Product:\")\n",
    "print(\"=\" * 20)\n",
    "for product in stock_per_product.collect():\n",
    "    format_string = f\"{{:<5}} | {{:<9}}\"\n",
    "    print(format_string.format(str(product[0]), str(round(product[1],2))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da852c07-c1d8-4763-8612-99bbc7133d51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
